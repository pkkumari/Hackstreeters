# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PVacsr7gjxSxLtCe8om2U5jGMenRnV8H
"""



import geopandas as gpd

geo_df = gpd.read_file("city_map.geojson")
geo_df.head()


import pandas as pd

# Load the uploaded file
disaster_df = pd.read_csv("disaster_events.csv")

# Preview the first few rows
disaster_df.head()

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point

# Make a point on the map
disaster_df['geometry'] = disaster_df.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)

# GeoDataFrame
disaster_gdf = gpd.GeoDataFrame(disaster_df, geometry='geometry', crs='EPSG:4326')


# Check first few rows
disaster_gdf.head()

# Change location data units
disaster_gdf = disaster_gdf.to_crs(epsg=3857)
geo_df = geo_df.to_crs(epsg=3857)

disaster_gdf['timestamp'] = pd.to_datetime(disaster_gdf['date'])

# Group by timestamp, location, and disaster_type
zone_ts = (
    disaster_gdf
    .groupby(['timestamp', 'location', 'disaster_type'])
    .size()
    .reset_index(name='count')
)

# Pivot to get disaster_type as columns
pivot_df = (
    zone_ts
    .pivot_table(index=['timestamp', 'location'],
                 columns='disaster_type',
                 values='count',
                 fill_value=0)
    .reset_index()
)

zone_c = pivot_df[pivot_df['location'] == 'Zone C'].drop(columns='location')
zone_c = zone_c.set_index('timestamp').resample('1H').sum().fillna(0)
zone_c.head()

# Create an empty list to store expanded disaster records by hour
expanded_rows = []

# Loop through each disaster event in Zone C
for _, row in disaster_gdf[disaster_gdf['location'] == 'Zone C'].iterrows():

    # Convert the disaster start time to datetime format
    start = pd.to_datetime(row['date'])

    # For each hour in the disaster duration
    for h in range(int(row['duration_hours'])):

        # Create a timestamp for each hour of impact
        timestamp = start + pd.Timedelta(hours=h)

        # Append the hourly record with same severity/casualty/economic loss
        expanded_rows.append({
            'timestamp': timestamp,  # Time this record applies to
            'disaster_type': row['disaster_type'],  # Type of disaster
            'severity': row['severity'],  # Severity level
            'casualties': row['casualties'],  # Number of casualties
            'economic_loss': row['economic_loss_million_usd']  # Economic loss
        })

# Convert the expanded list of records into a new DataFrame
expanded_df = pd.DataFrame(expanded_rows)

expanded_df

# Step 1: Pivot severity
sev = expanded_df.pivot_table(index='timestamp', columns='disaster_type', values='severity', aggfunc='sum')
sev.columns = ['sev_' + col for col in sev.columns]

# Step 2: Pivot casualties
cas = expanded_df.pivot_table(index='timestamp', columns='disaster_type', values='casualties', aggfunc='sum')
cas.columns = ['cas_' + col for col in cas.columns]

# Step 3: Pivot economic loss
eco = expanded_df.pivot_table(index='timestamp', columns='disaster_type', values='economic_loss', aggfunc='sum')
eco.columns = ['eco_' + col for col in eco.columns]

# Step 4: Merge all
zone_c_all = pd.concat([sev, cas, eco], axis=1).fillna(0)

# Optional: Check result
zone_c_all.head()

from sklearn.preprocessing import MinMaxScaler
import numpy as np

# 1. Normalize
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(zone_c_all)

# 2. Convert to DataFrame (optional, for reference)
scaled_df = pd.DataFrame(scaled_data, columns=zone_c_all.columns, index=zone_c_all.index)

# 3. Sequence ë§Œë“¤ê¸°
input_window = 6  # 6ì‹œê°„ ìž…ë ¥
output_window = 1  # 1ì‹œê°„ ì˜ˆì¸¡

X, y = [], []

for i in range(len(scaled_df) - input_window - output_window):
    X.append(scaled_df.iloc[i:i+input_window].values)
    y.append(scaled_df.iloc[i+input_window:i+input_window+output_window].values)

X = np.array(X)  # shape: (samples, 6, features)
y = np.array(y)  # shape: (samples, 1, features)

print("X shape:", X.shape)
print("y shape:", y.shape)

from tensorflow import keras
from keras import layers, models


model = keras.models.Sequential()
model.add(keras.layers.LSTM(64, input_shape=(input_window, X.shape[2]), return_sequences=False))
model.add(keras.layers.Dense(y.shape[2]))  # ì˜ˆì¸¡í•  feature ìˆ˜ë§Œí¼ ì¶œë ¥

model.compile(optimizer='adam', loss='mse')
model.summary()

# LSTM learning
model.fit(X, y[:, 0, :], epochs=20, batch_size=32)

# Find the most recent timestamp when a flood occurred (based on severity > 0)
latest_flood_time = zone_c_all[zone_c_all['sev_flood'] > 0].index[-1]

# Extract the 6-hour window prior to the latest flood for prediction input
X_recent_flood = scaled_df.loc[
    latest_flood_time - pd.Timedelta(hours=6):
    latest_flood_time - pd.Timedelta(hours=1)
].values

# Reshape the input to match LSTM expected shape: (samples, time_steps, features)
X_recent_flood = X_recent_flood.reshape(1, 6, 15)

# Run prediction using the trained LSTM model
y_pred = model.predict(X_recent_flood)

# Inverse transform the prediction to return to original scale
y_pred_original = scaler.inverse_transform(y_pred)

# Convert the prediction result into a readable DataFrame format
predicted_df = pd.DataFrame(y_pred_original, columns=zone_c_all.columns)

# Print the predicted disaster outcomes sorted by severity, casualties, or loss
print("Predicted Disaster Situation in 1 Hour After Latest Flood:\n")
print(predicted_df.T.sort_values(by=0, ascending=False))

""""Severe flood conditions are expected to persist, resulting in high economic losses ($287K) and casualties (27+ victims).
Additionally, there is a moderate risk of secondary disasters, including industrial accidents, fires, and earthquakes within the next hour.
Immediate evacuation and resource reallocation in Zone C are recommended."
"""

import pandas as pd
from collections import defaultdict

# Ensure datetime format
disaster_df['date'] = pd.to_datetime(disaster_df['date'])

# Sort chronologically
disaster_df = disaster_df.sort_values('date')

# Store cascading patterns
cascades = defaultdict(list)

# Time window to search for follow-up disasters (e.g., 6 hours after flood)
window_hours = 6

# Find all Zone C flood events
zone_c_floods = disaster_df[
    (disaster_df['location'] == 'Zone C') &
    (disaster_df['disaster_type'] == 'flood')
]

# Loop through each Zone C flood event
for _, flood_event in zone_c_floods.iterrows():
    flood_time = flood_event['date']

    # Find all events that occurred within the next 6 hours
    future_events = disaster_df[
        (disaster_df['date'] > flood_time) &
        (disaster_df['date'] <= flood_time + pd.Timedelta(hours=window_hours)) &
        (disaster_df['location'] != 'Zone C')  # exclude self-zone
    ]

    # Record the affected zones and disaster types
    for _, e in future_events.iterrows():
        key = f"{e['location']} - {e['disaster_type']}"
        cascades[key].append(1)

# Count frequencies
pattern_summary = {k: len(v) for k, v in cascades.items()}
pattern_df = pd.DataFrame.from_dict(pattern_summary, orient='index', columns=['count'])
pattern_df = pattern_df.sort_values(by='count', ascending=False)

# Display results
print("Cascading Patterns After Zone C Floods:\n")
print(pattern_df)

# Recommend safest zone (least frequent or not observed)
all_zones = {'Zone A', 'Zone B', 'Zone D'}
affected_zones = {k.split(' - ')[0] for k in pattern_df.index}
safe_zones = all_zones - affected_zones

print("\nRecommended Safe Zones (No recorded impact after C-floods):")
print(safe_zones if safe_zones else "No fully safe zone found. Use lowest risk zone based on count.")

"""â€œOur AI aligns all predictions to the latest flood in Zone C.
It forecasts not only what happens within Zone C,
but also what could cascade across other zones during the critical next hour.
Thatâ€™s how we generate real-time, citywide evacuation strategies.â€

"We placed the highest weight (0.6) on casualties,
recognizing that saving lives is the top priority in any crisis.
Our AI system reflects this value by prioritizing human impact over economic cost."
"""

# ê³µí†µ í•¨ìˆ˜ë¡œ ë¬¶ì–´ë„ ë˜ê³ , ì¼ë‹¨ Zone Aë¶€í„° ì˜ˆì‹œë¡œ í•´ì¤„ê²Œ
def build_zone_timeseries(zone_name):
    df = disaster_gdf[disaster_gdf['location'] == zone_name].copy()
    df = df.set_index('date')
    df.index = pd.to_datetime(df.index)

    # ê° ìž¬ë‚œë³„ ìˆ˜ì¹˜ pivot
    sev = df.pivot_table(index='date', columns='disaster_type', values='severity', aggfunc='sum')
    cas = df.pivot_table(index='date', columns='disaster_type', values='casualties', aggfunc='sum')
    eco = df.pivot_table(index='date', columns='disaster_type', values='economic_loss_million_usd', aggfunc='sum')

    # ì‹œê°„ ê°„ê²© ë§žì¶”ê¸°
    sev = sev.resample('1H').sum().fillna(0)
    cas = cas.resample('1H').sum().fillna(0)
    eco = eco.resample('1H').sum().fillna(0)

    # ì»¬ëŸ¼ëª… í†µì¼
    sev.columns = ['sev_' + col for col in sev.columns]
    cas.columns = ['cas_' + col for col in cas.columns]
    eco.columns = ['eco_' + col for col in eco.columns]

    # ìµœì¢… í•©ì¹˜ê¸°
    zone_df = pd.concat([sev, cas, eco], axis=1).fillna(0)
    return zone_df

zone_a_all = build_zone_timeseries('Zone A')
zone_b_all = build_zone_timeseries('Zone B')
zone_d_all = build_zone_timeseries('Zone D')

def get_recent_input_safe(df, ref_time, window=6):
    try:
        subset = df.loc[ref_time - pd.Timedelta(hours=window): ref_time - pd.Timedelta(hours=1)]
        if subset.shape[0] < window:
            print("â—ï¸Not enough data for", ref_time)
            return None
        return subset.values.reshape(1, window, -1)
    except:
        return None

X_a = get_recent_input_safe(zone_a_all, latest_flood_time)
if X_a is not None:
    y_a = model.predict(X_a)
    y_a = scaler.inverse_transform(y_a)

latest_common_time = min(
    zone_a_all.index.max(),
    zone_b_all.index.max(),
    zone_c_all.index.max(),
    zone_d_all.index.max()
)

print("Most recent common time for all zones:", latest_common_time)

input_window = 6
adjusted_time = pd.to_datetime("2015-09-14 22:00:00")

zone_preds = {}
valid_zones = {
    'Zone C': zone_c_all,
    'Zone A': zone_a_all,
    'Zone B': zone_b_all,
    'Zone D': zone_d_all
}

for zone_name, df in valid_zones.items():
    try:
        # ì‹œí€€ìŠ¤ ì¶”ì¶œ
        x = df.loc[
            adjusted_time - pd.Timedelta(hours=input_window):
            adjusted_time - pd.Timedelta(hours=1)
        ]

        if x.shape[0] < input_window:
            print(f"â—ï¸Skipping {zone_name}: Not enough data.")
            continue

        # ëª¨ë¸ ìž…ë ¥
        x_input = x.values.reshape(1, input_window, -1)

        # ì˜ˆì¸¡
        y_pred = model.predict(x_input)
        y_pred = scaler.inverse_transform(y_pred)

        # ì €ìž¥
        zone_preds[zone_name] = pd.DataFrame(y_pred, columns=df.columns)

    except Exception as e:
        print(f"âš ï¸ Error in {zone_name}: {e}")
        continue

def compute_risk_score(df):
    cas = df[[c for c in df.columns if c.startswith('cas_')]].sum(axis=1).values[0]
    sev = df[[c for c in df.columns if c.startswith('sev_')]].sum(axis=1).values[0]
    eco = df[[c for c in df.columns if c.startswith('eco_')]].sum(axis=1).values[0]
    return 0.6 * cas + 0.25 * sev + 0.15 * eco

zone_risks = {
    zone: compute_risk_score(pred_df) for zone, pred_df in zone_preds.items()
}

zone_risk_df = pd.DataFrame.from_dict(zone_risks, orient='index', columns=['Risk Score'])
zone_risk_df = zone_risk_df.sort_values(by='Risk Score', ascending=False)
print("\nðŸ”¥ Risk Score at common time (2015-09-14 22:00):")
print(zone_risk_df)

"""- Real-time flood detected in Zone C at 2:17 AM
- AI forecast indicates extremely high danger in Zone C within the next hour.
- LSTM model predicts high cascading risk for Zone B.
- Zone D remains the safest nearby region based on both real-time and historical data.
- Evacuation recommended: Zone C â†’ Zone D
"""

import folium
import pandas as pd

# 1. Load disaster data
df = pd.read_csv("disaster_events.csv")

# 2. Get latest flood in Zone C
zone_c_flood = df[(df["location"] == "Zone C") & (df["disaster_type"] == "flood")]
latest_c_flood = zone_c_flood.sort_values(by="date", ascending=False).iloc[0]

# 3. Get average coordinates of Zone D (safe zone)
zone_d_coords = df[df["location"] == "Zone D"][["latitude", "longitude"]].mean()

# 4. Create folium map centered in general region
m = folium.Map(location=[37.5, -122.1], zoom_start=10)

# 5. Add Zone C flood marker
folium.Marker(
    location=[latest_c_flood["latitude"], latest_c_flood["longitude"]],
    popup="âš ï¸ Zone C: Flood Event",
    icon=folium.Icon(color="blue", icon="tint", prefix="fa")
).add_to(m)

# 6. Add Zone D safe marker
folium.Marker(
    location=[zone_d_coords["latitude"], zone_d_coords["longitude"]],
    popup="âœ… Safe Zone D",
    icon=folium.Icon(color="green", icon="check", prefix="fa")
).add_to(m)

# 7. Add green evacuation route line
folium.PolyLine(
    locations=[
        [latest_c_flood["latitude"], latest_c_flood["longitude"]],
        [zone_d_coords["latitude"], zone_d_coords["longitude"]]
    ],
    color='green',
    weight=4,
    tooltip='Evacuation Route: Zone C â†’ Zone D'
).add_to(m)

# 8. Save map to file
m.save("simple_evacuation_map.html")


from geopy.distance import geodesic

c_coords = (latest_c_flood["latitude"], latest_c_flood["longitude"])
d_coords = (zone_d_coords["latitude"], zone_d_coords["longitude"])

distance_km = geodesic(c_coords, d_coords).km
print(f"Zone C â†’ Zone D ê±°ë¦¬: {distance_km:.2f} km")

"""â€œBased on both geospatial proximity and cascading disaster risk prediction,
Zone D emerges as the most viable evacuation destination.
Although it is nearly 57km away from the flood origin in Zone C,
the predicted drive time of under an hour ensures timely escape
before conditions worsen, making it both safe and realistic.â€
"""

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
from sklearn.neighbors import KNeighborsClassifier

# 1. Load disaster event data (ë¼ë²¨ë§ìš©)
df_event = pd.read_csv("disaster_events.csv")
df_event['geometry'] = df_event.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)
gdf_event = gpd.GeoDataFrame(df_event, geometry='geometry', crs='EPSG:4326')

# 2. Prepare training data for zone classification
X_train = df_event[['longitude', 'latitude']]
y_train = df_event['location']  # Zone A/B/C/D

# 3. Load social media data
df_social = pd.read_csv("social_media_stream.csv")
X_test = df_social[['longitude', 'latitude']]

# 4. Train a simple KNN to label social posts with nearest known zone
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)
zone_labels = knn.predict(X_test)

# 5. Add zone column to social data
df_social['zone'] = zone_labels

# âœ… Done! Now you know which zone each tweet came from
print(df_social[['text', 'zone']].head())

# Step 1. ê´€ì‹¬ ìžˆëŠ” í‚¤ì›Œë“œ í•„í„°ë§
keywords = ['flood', 'evacuate']
df_alerts = df_social[
    df_social['text'].str.contains('|'.join(keywords), case=False)
]

# Step 2. ê·¸ ì¤‘ ì‹¤ì œ Zone Cì—ì„œ ë°œìƒí•œ ë©”ì‹œì§€
zone_c_alerts = df_alerts[df_alerts['zone'] == 'Zone C']

# ê²°ê³¼ ë³´ê¸°
print("ðŸ§­ Total flood/evacuation-related tweets:", len(df_alerts))
print("ðŸ“ Of which from Zone C:", len(zone_c_alerts))
print(zone_c_alerts[['text', 'latitude', 'longitude', 'zone']])

# Define trust rules for keyword-based alerts
def assess_trust_level(text):
    text = text.lower()
    if "evacuate zone c" in text:
        return "high"
    elif "flood near river" in text:
        return "medium"
    else:
        return "low"

# Apply rule to each Zone C alert
zone_c_alerts["trust_level"] = zone_c_alerts["text"].apply(assess_trust_level)

# Count by trust level
print(zone_c_alerts["trust_level"].value_counts())

# Save Zone C alerts with trust level to CSV
zone_c_alerts.to_csv("zone_c_social_alerts_with_trust.csv", index=False)


import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
from sklearn.neighbors import BallTree
import numpy as np

# Load disaster event data (to extract zone info)
df_disaster = pd.read_csv("disaster_events.csv")
df_disaster = df_disaster[['latitude', 'longitude', 'location']].drop_duplicates()

# Convert to GeoDataFrame
gdf_disaster = gpd.GeoDataFrame(
    df_disaster,
    geometry=gpd.points_from_xy(df_disaster.longitude, df_disaster.latitude),
    crs="EPSG:4326"
)

# Load sensor data
df_sensor = pd.read_csv("sensor_readings.csv")
df_sensor["timestamp"] = pd.to_datetime(df_sensor["timestamp"])

# Convert sensor to GeoDataFrame
gdf_sensor = gpd.GeoDataFrame(
    df_sensor,
    geometry=gpd.points_from_xy(df_sensor.longitude, df_sensor.latitude),
    crs="EPSG:4326"
)

# Convert lat/lon to radians for BallTree
disaster_coords = np.radians(gdf_disaster[['latitude', 'longitude']])
sensor_coords = np.radians(gdf_sensor[['latitude', 'longitude']])

# Use BallTree to find nearest disaster event (and its zone)
tree = BallTree(disaster_coords, metric='haversine')
distances, indices = tree.query(sensor_coords, k=1)

# Assign closest zone to each sensor
gdf_sensor["zone"] = gdf_disaster.iloc[indices.flatten()].location.values

# âœ… Now filter Zone C sensors
sensor_c = gdf_sensor[gdf_sensor["zone"] == "Zone C"]

# Optional: Save to file
sensor_c.to_csv("sensor_zone_c_joined.csv", index=False)

import pandas as pd

# Load high-trust Zone C alerts
zone_c_alerts = pd.read_csv("zone_c_social_alerts_with_trust.csv")
zone_c_alerts["timestamp"] = pd.to_datetime(zone_c_alerts["timestamp"])
zone_c_alerts = zone_c_alerts[zone_c_alerts["trust_level"] == "high"]

# Load sensor data already joined with Zone info
sensor_zone_c_joined = pd.read_csv("sensor_zone_c_joined.csv")  # zone C only
sensor_zone_c_joined["timestamp"] = pd.to_datetime(sensor_zone_c_joined["timestamp"])

# Load weather and energy datasets
weather_df = pd.read_csv("weather_historical.csv")
weather_df["timestamp"] = pd.to_datetime(weather_df["timestamp"])

energy_df = pd.read_csv("energy_consumption.csv")
energy_df["timestamp"] = pd.to_datetime(energy_df["timestamp"])

# Step 1: Define Â±1 hour time windows
time_windows = []
for ts in zone_c_alerts["timestamp"]:
    time_windows.append((ts - pd.Timedelta(hours=1), ts + pd.Timedelta(hours=1)))

# Step 2: Filter sensor readings within time window + already in Zone C
sensor_near_alerts = pd.concat([
    sensor_zone_c_joined[
        (sensor_zone_c_joined["timestamp"] >= start) &
        (sensor_zone_c_joined["timestamp"] <= end)
    ]
    for start, end in time_windows
], ignore_index=True)

# Step 3: Filter weather and energy as-is (no location)
weather_near_alerts = pd.concat([
    weather_df[(weather_df["timestamp"] >= start) & (weather_df["timestamp"] <= end)]
    for start, end in time_windows
], ignore_index=True)

energy_near_alerts = pd.concat([
    energy_df[(energy_df["timestamp"] >= start) & (energy_df["timestamp"] <= end)]
    for start, end in time_windows
], ignore_index=True)

# Step 4: Save results
sensor_near_alerts.to_csv("zone_c_sensor_filtered.csv", index=False)
weather_near_alerts.to_csv("zone_c_weather_filtered.csv", index=False)
energy_near_alerts.to_csv("zone_c_energy_filtered.csv", index=False)

# Step 4: ìœ„ê¸° ì‹ ë¢°ë„ íŒë‹¨ í•¨ìˆ˜ (ë£° ê¸°ë°˜)
def estimate_crisis_reliability(sensor_df, weather_df, energy_df):
    result = []

    for ts in zone_c_alerts["timestamp"]:
        # ì‹œê°„ í•„í„°: Â±1ì‹œê°„
        t_start = ts - pd.Timedelta(hours=1)
        t_end = ts + pd.Timedelta(hours=1)

        # ì„¼ì„œ ë°ì´í„° í•„í„°
        nearby_sensors = sensor_df[(sensor_df["timestamp"] >= t_start) & (sensor_df["timestamp"] <= t_end)]
        flood_alert = (nearby_sensors["sensor_type"] == "flood") & (nearby_sensors["value"] > 0.8)

        # ë‚ ì”¨ ë°ì´í„° í•„í„°
        nearby_weather = weather_df[(weather_df["timestamp"] >= t_start) & (weather_df["timestamp"] <= t_end)]
        heavy_rain = (nearby_weather["rain_mm"] > 20).any()  # ì˜ˆì‹œ ì¡°ê±´

        # ì—ë„ˆì§€ ì†Œë¹„ ê°ì†Œ ì—¬ë¶€ (ì •ì „ ê°€ì •)
        nearby_energy = energy_df[(energy_df["timestamp"] >= t_start) & (energy_df["timestamp"] <= t_end)]
        early_energy = nearby_energy.head(3)["energy_kwh"].mean()
        late_energy = nearby_energy.tail(3)["energy_kwh"].mean()
        blackout = (late_energy < early_energy * 0.6)  # ì†Œë¹„ê°€ 40% ì´ìƒ ê°ì†Œ

        # ì¢…í•© ì‹ ë¢° íŒë‹¨
        trust_score = sum([flood_alert.any(), heavy_rain, blackout])
        if trust_score == 3:
            level = "High"
        elif trust_score == 2:
            level = "Medium"
        elif trust_score == 1:
            level = "Low"
        else:
            level = "Unlikely"

        result.append({
            "timestamp": ts,
            "sensor_confirmed": flood_alert.any(),
            "rain_confirmed": heavy_rain,
            "blackout": blackout,
            "trust_level": level
        })

    return pd.DataFrame(result)
